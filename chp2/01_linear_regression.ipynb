{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2c4b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from typing import Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70e30005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_forward(X_batch: ndarray,\n",
    "                              y_batch: ndarray,\n",
    "                              weights: Dict[str, ndarray]) -> Tuple[float, Dict[str, ndarray]]:\n",
    "    # input dimensions\n",
    "    assert X_batch.shape[0] == y_batch.shape[0]\n",
    "    assert X_batch.shape[1] == weights['W'].shape[0]\n",
    "    assert weights['B'].shape[0] == weights['B'].shape[1]\n",
    "    \n",
    "    # forward pass\n",
    "    N = np.dot(X_batch, weights['W'])\n",
    "    P = N + weights['B']\n",
    "    \n",
    "    # calculate loss\n",
    "    # method 1\n",
    "    loss = np.mean(np.power(y_batch - P, 2))\n",
    "    # method 2\n",
    "    # loss = np.sum(np.power(y_batch - P, 2))/(2*len(X_batch))\n",
    "    \n",
    "    # save forward pass info\n",
    "    forward_info: Dict[str, ndarray] = {}\n",
    "    forward_info['X'] = X_batch\n",
    "    forward_info['P'] = P\n",
    "    forward_info['N'] = N \n",
    "    forward_info['y'] = y_batch\n",
    "    \n",
    "    return loss, forward_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e9e9826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.03333333333333335\n",
      "Forward Pass: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': array([[0, 2],\n",
       "        [2, 5],\n",
       "        [3, 4]]),\n",
       " 'P': array([[0.3],\n",
       "        [1. ],\n",
       "        [1.1]]),\n",
       " 'N': array([[0.2],\n",
       "        [0.9],\n",
       "        [1. ]]),\n",
       " 'y': array([[0],\n",
       "        [1],\n",
       "        [1]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [0,2],\n",
    "    [2,5],\n",
    "    [3,4]\n",
    "     ])\n",
    "\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1]\n",
    "     ])\n",
    "\n",
    "w = {\n",
    "    'W': np.array([\n",
    "        [0.2],\n",
    "        [0.1]\n",
    "        ]),\n",
    "    'B': np.array([[0.1]])\n",
    "}\n",
    "\n",
    "loss, forward_info = linear_regression_forward(X, y, w)\n",
    "print('Loss: ', loss)\n",
    "print('Forward Pass: ');display(forward_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d98d40cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': array([[0.6],\n",
       "        [2. ]]),\n",
       " 'B': array([[0.8]])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def loss_gradients(forward_info: Dict[str, ndarray], weights: Dict[str, ndarray]) -> Dict[str, ndarray]:\n",
    "    batch_size = forward_info['X'].shape[0] # number of samples\n",
    "    \n",
    "    # find grads: dLdB, dLdW\n",
    "    \n",
    "    # grad of loss wrt p -> using squared error: (y-P) ^ 2\n",
    "    dLdP = -2 * (forward_info['y'] - forward_info['P'])\n",
    "    # for mean squared error, grad of loss wrt p -> using mean squared error: 1/2(y-P) ^ 2\n",
    "    # dLdP = forward_info['P'] - forward_info['y']\n",
    "    \n",
    "    # intermediate derivatives\n",
    "    dPdN = np.ones_like(forward_info['N'])\n",
    "    dPdB = np.ones_like(weights['B'])\n",
    "    dLdN = dLdP * dPdN \n",
    "    dNdW = np.transpose(forward_info['X'], (1, 0))\n",
    "    \n",
    "    # final derivatives of w, b\n",
    "    dLdW = np.dot(dNdW, dLdN)\n",
    "    dLdB = (dLdP * dPdB).sum(axis=0, keepdims=True)\n",
    "    \n",
    "    # save the loss\n",
    "    loss_gradients: Dict[str, ndarray] = {}\n",
    "    loss_gradients['W'] = dLdW\n",
    "    loss_gradients['B'] = dLdB\n",
    "    \n",
    "    return loss_gradients\n",
    "\n",
    "loss_g = loss_gradients(forward_info, w)\n",
    "display(loss_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dlfsvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
